{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3333e18-331a-4777-ba2d-a1348512d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from econml.dml import LinearDML\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output (e.g., convergence warnings in high-dim logistic reg)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56cf4ef1-e964-4fd6-a6af-164bfface420",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalEstimators:\n",
    "\n",
    "# A helper class to run various causal inference methods:\n",
    "# 1. Naive (Unadjusted Difference in Means)\n",
    "# 2. Regression Adjustment (Linear/Logistic)\n",
    "# 3. IPTW (Inverse Probability of Treatment Weighting)\n",
    "# 4. PSM (Propensity Score Matching 1:1)\n",
    "# 5. DML (Double Machine Learning)\n",
    "\n",
    "    def __init__(self, X, T, Y, outcome_type= 'continuous'):\n",
    "        self.X = X\n",
    "        self.T = T\n",
    "        self.Y = Y\n",
    "        self.outcome_type = outcome_type\n",
    "        # Create a DataFrame for easier handling in naive methods\n",
    "        self.df = pd.DataFrame(X)\n",
    "        self.df['T'] = T\n",
    "        self.df['Y'] = Y\n",
    "    def run_naive(self):\n",
    "    \n",
    "        treated = self.df[self.df['T'] == 1]['Y'].mean()\n",
    "        control = self.df[self.df['T'] == 0]['Y'].mean()\n",
    "        return treated - control\n",
    "\n",
    "    def run_regression_adjustment(self):\n",
    "\n",
    "        # Combine T and X into one feature matrix\n",
    "        features = np.column_stack((self.T, self.X))\n",
    "        model = LinearRegression()\n",
    "        model.fit(features, self.Y)\n",
    "        # The coefficient of T (index 0) is the additive effect\n",
    "        return model.coef_[0]\n",
    "\n",
    "    def run_iptw(self):\n",
    "        # Estimate Propensity Scores\n",
    "        ps_model = LogisticRegression(solver='liblinear', max_iter=2000)\n",
    "        ps_model.fit(self.X, self.T)\n",
    "        ps = ps_model.predict_proba(self.X)[:, 1]\n",
    "        \n",
    "        # Clip to prevent division by zero or extreme weights (common in small N)\n",
    "        ps = np.clip(ps, 0.05, 0.95)\n",
    "        # Calculate weights: 1/PS for treated, 1/(1-PS) for control\n",
    "        weights = np.where(self.T == 1, 1/ps, 1/(1-ps))\n",
    "        model = LinearRegression()\n",
    "        model.fit(self.T.reshape(-1, 1), self.Y, sample_weight=weights)\n",
    "        return model.coef_[0]\n",
    "\n",
    "    def run_psm(self):\n",
    "        # Estimate Propensity Scores\n",
    "        ps_model = LogisticRegression(solver='liblinear', max_iter=2000)\n",
    "        ps_model.fit(self.X, self.T)\n",
    "        ps = ps_model.predict_proba(self.X)[:, 1]\n",
    "        \n",
    "        treated_idx = np.where(self.T == 1)[0]\n",
    "        control_idx = np.where(self.T == 0)[0]\n",
    "        \n",
    "        # Safety check for separation\n",
    "        if len(control_idx) == 0 or len(treated_idx) == 0:\n",
    "            return np.nan\n",
    "        # Match each treated unit to nearest control unit based on PS\n",
    "        nbrs = NearestNeighbors(n_neighbors=1).fit(ps[control_idx].reshape(-1, 1))\n",
    "        distances, indices = nbrs.kneighbors(ps[treated_idx].reshape(-1, 1))\n",
    "        matched_control_idx = control_idx[indices.flatten()]\n",
    "        \n",
    "        # Difference in Means of matched pairs (Risk Difference for binary)\n",
    "        return np.mean(self.Y[treated_idx]) - np.mean(self.Y[matched_control_idx])\n",
    "    \n",
    "    def run_dml(self):\n",
    "        # Use shallow trees (depth 2) to prevent overfitting on small N=500\n",
    "        y_model = GradientBoostingRegressor(n_estimators=50, max_depth=2,\n",
    "        random_state=42)\n",
    "        t_model = GradientBoostingClassifier(n_estimators=50, max_depth=2,\n",
    "        random_state=42)\n",
    "    \n",
    "        # Set cv=3 for small sample size (5-fold might be too thin)\n",
    "        est = LinearDML(model_y=y_model,\n",
    "                        model_t=t_model,\n",
    "                        discrete_treatment=True,\n",
    "                        cv=3,\n",
    "                        random_state=42)\n",
    "        est.fit(self.Y, self.T, X=self.X)\n",
    "        # Return Average Treatment Effect\n",
    "        return est.effect(self.X).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "619d196e-9590-43a2-8606-5cf338408c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Method 1 Naive estimates\n",
    "def run_naive(self):\n",
    "    \n",
    "    treated = self.df[self.df['T'] == 1]['Y'].mean()\n",
    "    control = self.df[self.df['T'] == 0]['Y'].mean()\n",
    "    return treated - control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f091531-0a53-47da-9af5-a99f0b92b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Method 2 Linear regression with covariates adjusted\n",
    "def run_regression_adjustment(self):\n",
    "\n",
    "    # Combine T and X into one feature matrix\n",
    "    features = np.column_stack((self.T, self.X))\n",
    "    model = LinearRegression()\n",
    "    model.fit(features, self.Y)\n",
    "    # The coefficient of T (index 0) is the additive effect\n",
    "    return model.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "686995a6-defc-46ec-8d44-210e54848539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Method 3 IPTW (Logistic PS) \n",
    "def run_iptw(self):\n",
    "# Estimate Propensity Scores\n",
    "    ps_model = LogisticRegression(solver='liblinear', max_iter=2000)\n",
    "    ps_model.fit(self.X, self.T)\n",
    "    ps = ps_model.predict_proba(self.X)[:, 1]\n",
    "    \n",
    "    # Clip to prevent division by zero or extreme weights (common in small N)\n",
    "    ps = np.clip(ps, 0.05, 0.95)\n",
    "    # Calculate weights: 1/PS for treated, 1/(1-PS) for control\n",
    "    weights = np.where(self.T == 1, 1/ps, 1/(1-ps))\n",
    "    model = LinearRegression()\n",
    "    model.fit(self.T.reshape(-1, 1), self.Y, sample_weight=weights)\n",
    "    return model.coef_[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea37be18-ad10-40a7-8b8e-70f93e29858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Method 4 PSM (1:1 Nearest Neighbor) \n",
    "def run_psm(self):\n",
    "    # Estimate Propensity Scores\n",
    "    ps_model = LogisticRegression(solver='liblinear', max_iter=2000)\n",
    "    ps_model.fit(self.X, self.T)\n",
    "    ps = ps_model.predict_proba(self.X)[:, 1]\n",
    "    \n",
    "    treated_idx = np.where(self.T == 1)[0]\n",
    "    control_idx = np.where(self.T == 0)[0]\n",
    "    \n",
    "    # Safety check for separation\n",
    "    if len(control_idx) == 0 or len(treated_idx) == 0:\n",
    "        return np.nan\n",
    "    # Match each treated unit to nearest control unit based on PS\n",
    "    nbrs = NearestNeighbors(n_neighbors=1).fit(ps[control_idx].reshape(-1, 1))\n",
    "    distances, indices = nbrs.kneighbors(ps[treated_idx].reshape(-1, 1))\n",
    "    matched_control_idx = control_idx[indices.flatten()]\n",
    "    \n",
    "    # Difference in Means of matched pairs (Risk Difference for binary)\n",
    "    return np.mean(self.Y[treated_idx]) - np.mean(self.Y[matched_control_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b580089-8dff-4d86-9588-6d6abdbad95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Method 5 DML (Gradient Boosting) \n",
    "def run_dml(self):\n",
    "# Use shallow trees (depth 2) to prevent overfitting on small N=500\n",
    "    y_model = GradientBoostingRegressor(n_estimators=50, max_depth=2,\n",
    "    random_state=42)\n",
    "    t_model = GradientBoostingClassifier(n_estimators=50, max_depth=2,\n",
    "    random_state=42)\n",
    "\n",
    "# Set cv=3 for small sample size (5-fold might be too thin)\n",
    "    est = LinearDML(model_y=y_model,\n",
    "                    model_t=t_model,\n",
    "                    discrete_treatment=True,\n",
    "                    cv=3,\n",
    "                    random_state=42)\n",
    "    est.fit(self.Y, self.T, X=self.X)\n",
    "    # Return Average Treatment Effect\n",
    "    return est.effect(self.X).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f318c697-4fa8-4097-8af3-f782d84be8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation function\n",
    "def generate_rare_disease_data(n=500, p=100, setting='continuous',seed=123):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # 1. Generate Covariates (100 variables)\n",
    "    X = np.random.normal(0, 1, size=(n, p))\n",
    "    \n",
    "    # 2. Define the &quot;True&quot; Confounding Mechanism (The Nuisance Function)\n",
    "    # Only the first 5 variables (indices 0-4) actually matter.\n",
    "    # Interaction: X0 * X1\n",
    "    # Non-linear: X2 squared\n",
    "    # Linear: X3, X4\n",
    "    nuisance_term = 0.5 * X[:,0] * X[:,1] + 0.4 * (X[:,2]**2) + 0.3 * X[:,3] + 0.2 * X[:,4]\n",
    "    # 3. Treatment Assignment (Propensity)\n",
    "    # P(T=1 | X) depends on the nuisance term\n",
    "    logit_p = -0.5 + nuisance_term\n",
    "    prob_t = 1 / (1 + np.exp(-logit_p))\n",
    "    T = np.random.binomial(1, prob_t)\n",
    "    TRUE_EFFECT = 1000.0 # True Causal Effect ($1000)\n",
    "    \n",
    "    # Outcome depends on T, the nuisance term (confounding), and noise\n",
    "    # Note: We scale the nuisance term to ensure confounding is strong enough to bias naive results \n",
    "    confounding_effect = 1000 * nuisance_term\n",
    "    \n",
    "    Y = 2000 + (TRUE_EFFECT * T) + confounding_effect + np.random.normal(0,\n",
    "    200, n)\n",
    "    \n",
    "    return X, T, Y, TRUE_EFFECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37a87c4d-39d6-4929-90e3-42213163d358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "RARE DISEASE SIMULATION STUDY (N=500, P=100)\n",
      "Features: High Dimensionality, Sparsity, Non-Linearity\n",
      "=========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"=========================================================\")\n",
    "    print(\"RARE DISEASE SIMULATION STUDY (N=500, P=100)\")\n",
    "    print(\"Features: High Dimensionality, Sparsity, Non-Linearity\")\n",
    "    print(\"=========================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06a025f2-8333-41b3-9460-51a5b3b28257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario A: Continuous Outcome (e.g., Total Pay)\n",
      "True Causal Effect: 1000.00\n",
      "-----------------------------------------------------------------\n",
      "Method                         | Estimate        | Bias      \n",
      "-----------------------------------------------------------------\n",
      "Naive (Unadjusted)             | 1620.98         | 620.98    \n",
      "Regression (Linear):           | 1457.88         | 457.88    \n",
      "IPTW (Logistic PS)             | 1500.66         | 500.66    \n",
      "PSM (1:1 Nearest Neighbor)     | 1428.08         | 428.08    \n",
      "DML (Gradient Boosting)        | 1312.57         | 312.57    \n",
      "\n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1. CONTINUOUS OUTCOME ---\n",
    "X, T, Y, true_eff = generate_rare_disease_data(n=500, p=100, setting='continuous')\n",
    "sim = CausalEstimators(X, T, Y, outcome_type='continuous')\n",
    "\n",
    "print(f\"Scenario A: Continuous Outcome (e.g., Total Pay)\")\n",
    "print(f\"True Causal Effect: {true_eff:.2f}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Method':<30} | {'Estimate':<15} | {'Bias':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "results_continuous = {\n",
    "\"Naive (Unadjusted)\": sim.run_naive(),\n",
    "\"Regression (Linear):\": sim.run_regression_adjustment(),\n",
    "\"IPTW (Logistic PS)\": sim.run_iptw(),\n",
    "\"PSM (1:1 Nearest Neighbor)\": sim.run_psm(),\n",
    "\"DML (Gradient Boosting)\": sim.run_dml()\n",
    "}\n",
    "\n",
    "for method, val in results_continuous.items():\n",
    "    bias = val - true_eff\n",
    "    print(f\"{method:<30} | {val:<15.2f} | {bias:<10.2f}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*65 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
